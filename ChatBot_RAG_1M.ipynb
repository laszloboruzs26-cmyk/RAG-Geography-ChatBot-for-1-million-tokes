{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a9d0a3",
   "metadata": {},
   "source": [
    "# RAG Chatbot (Scales to ~1M Tokens of Source Text)\n",
    "\n",
    "This notebook upgrades your simple employee-policy chatbot into a **RAG** system that can index and query a much larger dataset (e.g., ~1,000,000 tokens across many files).\n",
    "\n",
    "## What changes vs your original notebook\n",
    "- Uses a **persistent vector database (Chroma)** on disk.\n",
    "- Adds an **ingestion/indexing pipeline** that can handle large corpora: load → chunk → embed → store (batched).\n",
    "- Supports **incremental re-runs** (won't duplicate already-indexed chunks).\n",
    "- Removes Google Colab-only bits (no `/content`, no `userdata`).\n",
    "\n",
    "## Folder structure (recommended)\n",
    "```\n",
    "project/\n",
    "  data/                 # put your PDFs/TXTs/MDs here\n",
    "  chroma_db/            # created automatically\n",
    "  ChatBot_RAG_1M.ipynb\n",
    "```\n",
    "\n",
    "> “1 million tokens” refers to the total text size across all documents after tokenization, not a single prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd260f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "%pip -q install -U langchain langchain-openai langchain-community langchain-chroma chromadb pypdf tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ====== CONFIG ======\n",
    "DATA_DIR = Path(\"data\")            # put your large dataset here\n",
    "CHROMA_DIR = Path(\"chroma_db\")     # persistent vector DB folder\n",
    "COLLECTION_NAME = \"employee_policy_kb\"\n",
    "\n",
    "# Retrieval tuning\n",
    "TOP_K = 8\n",
    "USE_MMR = True  # MMR increases diversity of retrieved chunks; often improves answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e8be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API key\n",
    "# Set in your shell before starting VS Code:\n",
    "#   export OPENAI_API_KEY=\"...\"\n",
    "# (Windows PowerShell):\n",
    "#   setx OPENAI_API_KEY \"...\"  # then restart terminal\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Missing OPENAI_API_KEY environment variable.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b6118",
   "metadata": {},
   "source": [
    "## 1) Load and chunk documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ce5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_documents(data_dir: Path):\n",
    "    \"\"\"Load supported files from a folder.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    if not data_dir.exists():\n",
    "        raise FileNotFoundError(f\"DATA_DIR not found: {data_dir.resolve()}\")\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    # PDFs\n",
    "    pdf_loader = DirectoryLoader(\n",
    "        str(data_dir),\n",
    "        glob=\"**/*.pdf\",\n",
    "        loader_cls=PyPDFLoader,\n",
    "        show_progress=True\n",
    "    )\n",
    "    docs.extend(pdf_loader.load())\n",
    "\n",
    "    # Plain text / markdown\n",
    "    txt_loader = DirectoryLoader(\n",
    "        str(data_dir),\n",
    "        glob=\"**/*.[tm][xd]\",  # .txt and .md\n",
    "        loader_cls=lambda p: TextLoader(p, encoding=\"utf-8\"),\n",
    "        show_progress=True\n",
    "    )\n",
    "    docs.extend(txt_loader.load())\n",
    "\n",
    "    return docs\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9198b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_documents(DATA_DIR)\n",
    "print(f\"Loaded {len(docs)} document pages/files\")\n",
    "\n",
    "splits = splitter.split_documents(docs)\n",
    "print(f\"Created {len(splits)} chunks\")\n",
    "\n",
    "print(splits[0].metadata)\n",
    "print(splits[0].page_content[:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9814eec3",
   "metadata": {},
   "source": [
    "## 2) (Optional) Estimate token size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38344f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def estimate_tokens(texts, model_encoding=\"cl100k_base\"):\n",
    "    enc = tiktoken.get_encoding(model_encoding)\n",
    "    return sum(len(enc.encode(t)) for t in texts)\n",
    "\n",
    "total_tokens_est = estimate_tokens([d.page_content for d in splits])\n",
    "print(f\"Estimated tokens across chunks: {total_tokens_est:,}\")\n",
    "print(f\"Approx words (very rough): {int(total_tokens_est * 0.75):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34553cfa",
   "metadata": {},
   "source": [
    "## 3) Build / Update the Vector Database (persistent, incremental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c80a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "def chunk_id(doc) -> str:\n",
    "    \"\"\"Deterministic id so re-ingesting doesn't create duplicates.\"\"\"\n",
    "    src = str(doc.metadata.get(\"source\", \"\"))\n",
    "    page = str(doc.metadata.get(\"page\", \"\"))\n",
    "    content = doc.page_content\n",
    "    return hashlib.sha1((src + \"|\" + page + \"|\" + content).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def upsert_documents(vectorstore: Chroma, documents, batch_size=128):\n",
    "    \"\"\"Add documents in batches, skipping chunks already present.\"\"\"\n",
    "    ids = [chunk_id(d) for d in documents]\n",
    "\n",
    "    existing = set()\n",
    "    for i in range(0, len(ids), 2000):\n",
    "        batch_ids = ids[i:i+2000]\n",
    "        got = vectorstore.get(ids=batch_ids, include=[])\n",
    "        existing.update(got.get(\"ids\", []))\n",
    "\n",
    "    to_add_docs, to_add_ids = [], []\n",
    "    for d, id_ in zip(documents, ids):\n",
    "        if id_ not in existing:\n",
    "            d.metadata = dict(d.metadata)\n",
    "            d.metadata[\"chunk_id\"] = id_\n",
    "            to_add_docs.append(d)\n",
    "            to_add_ids.append(id_)\n",
    "\n",
    "    print(f\"Already indexed: {len(existing):,}\")\n",
    "    print(f\"New chunks to add: {len(to_add_docs):,}\")\n",
    "\n",
    "    for i in range(0, len(to_add_docs), batch_size):\n",
    "        vectorstore.add_documents(to_add_docs[i:i+batch_size], ids=to_add_ids[i:i+batch_size])\n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"...added {min(i+batch_size, len(to_add_docs)):,}/{len(to_add_docs):,}\")\n",
    "\n",
    "    vectorstore.persist()\n",
    "    return len(to_add_docs)\n",
    "\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "vectorstore = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=str(CHROMA_DIR),\n",
    ")\n",
    "\n",
    "added = upsert_documents(vectorstore, splits, batch_size=128)\n",
    "print(f\"Vector DB ready. Added {added:,} new chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eac310",
   "metadata": {},
   "source": [
    "## 4) Retrieval + Chat (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6374b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a helpful assistant. Answer ONLY using the provided context. \"\n",
    "     \"If the answer is not in the context, say you don't know. \"\n",
    "     \"Be concise and, when possible, cite the source filenames/pages.\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{question}\")\n",
    "])\n",
    "\n",
    "if USE_MMR:\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": TOP_K, \"fetch_k\": max(20, TOP_K*3)}\n",
    "    )\n",
    "else:\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "def format_context(docs):\n",
    "    lines = []\n",
    "    for d in docs:\n",
    "        src = Path(d.metadata.get(\"source\",\"\")).name\n",
    "        page = d.metadata.get(\"page\", None)\n",
    "        loc = f\"{src}\" + (f\" p.{page}\" if page is not None else \"\")\n",
    "        lines.append(f\"[Source: {loc}]\\n{d.page_content}\")\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "def ask_question(question: str):\n",
    "    docs = retriever.invoke(question)\n",
    "    context = format_context(docs)\n",
    "    resp = llm.invoke(prompt.format_messages(context=context, question=question))\n",
    "    return resp.content, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f440b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, sources = ask_question(\"What is the purpose of the UK workplace employment policy?\")\n",
    "print(answer)\n",
    "print(\"\\n--- Retrieved sources ---\")\n",
    "for s in sources[:5]:\n",
    "    print(Path(s.metadata.get(\"source\",\"\")).name, \"page:\", s.metadata.get(\"page\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2ea6a1",
   "metadata": {},
   "source": [
    "## 5) Simple CLI loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939eb42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    q = input(\"\\nYou: \").strip()\n",
    "    if q.lower() in {\"exit\",\"quit\"}:\n",
    "        break\n",
    "    answer, _ = ask_question(q)\n",
    "    print(\"\\nBot:\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
